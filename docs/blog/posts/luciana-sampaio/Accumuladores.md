---
date:
  created: 2025-03-25
authors:
  - luciana
categories:
  - Luciana Sampaio
comments: true
slug: acumuladores
tags:
  - Engenharia de Dados
  - pySpark
  - How-to
---

# Acumuladores no PySpark: O que sÃ£o e como usÃ¡-los?

Imagine que vocÃª estÃ¡ cozinhando um prato incrÃ­vel, mas precisa contar quantas vezes adicionou tempero. Agora, multiplique isso por milhares de nÃ³s trabalhando juntos! ğŸ²ğŸ”¥

No mundo do PySpark, os acumuladores fazem exatamente isso: ajudam a contar e somar valores sem causar caos no cluster!

<!-- more -->

![alt text](../../../images/blog/luciana/image-3acum.png)
# ğŸ¯ O que sÃ£o acumuladores?
- SÃ£o variÃ¡veis especiais que sÃ³ podem ser incrementadas pelos trabalhadores (workers) e lidas - pelo nÃ³ principal (driver).
- Perfeitos para agregar mÃ©tricas, rastrear estatÃ­sticas globais e monitorar processos sem - sobrecarregar o Spark.
- Como PySpark trabalha em ambiente distribuÃ­do, os acumuladores garantem que informaÃ§Ãµes - essenciais sejam coletadas sem criar um pandemÃ´nio na rede!


## Como usar o Acumulador no Pyspark. 

VocÃª pode definir um acumulador no PySpark usando o mÃ©todo `accumulator` disponÃ­vel no `SparkContext`.

![alt text](../../../images/blog/luciana/acum3.png)


# Por que isso Ã© Ãºtil?

- Monitoramento de jobs â€“ Descubrir quantos registros foram processados sem criar gargalos.
- Contagem de erros â€“ Conte exceÃ§Ãµes sem precisar embaralhar (shuffle) os dados.
- MÃ©tricas de desempenho â€“ MeÃ§a estatÃ­sticas globais sem prejudicar a performance do cluster.
  
---


![alt text](../../../images/blog/luciana/acum2.png)


>  Ã‰ importante notar que dentro de transformaÃ§Ãµes como map e filter, que retornam um novo RDD, o valor do acumulador serÃ¡ contado para cada registro do novo RDD. Assim Ã© muito comum o uso de acumuladores dentro de aÃ§Ãµes como o foreach.

Exemplo abaixo:

![alt text](../../../images/blog/luciana/acum4.png)

## Acumuladores Embutidos:
O Spark fornece vÃ¡rios tipos de acumuladores embutidos:
- Long Accumulator: Usado para agregar valores de tipo Longo.
- Dobro Accumulator: Usado para agregar valores de tipo duplo.
- ColeÃ§Ã£o Accumulator: Acumula elementos em uma coleÃ§Ã£o no nÃ³ do driver.
- Esses acumuladores sÃ£o projetados especificamente para lidar com seus respectivos tipos de - dados (Longo, Duplo ou ColeÃ§Ã£o) e sÃ£o otimizados para desempenho no ambiente distribuÃ­do.


## Resumindo 

- Acumuladores sÃ£o Ãºteis para depuraÃ§Ã£o e monitoramento de trabalhos do Spark.

### Segue meus contatos no que precisar. 

 ğŸŒ [LinkedIn](https://www.linkedin.com/in/luciana-sampaio/)  
  *O lugar onde pareÃ§o super profissional e sÃ©ria. ğŸ˜‰*

 ğŸ™ [GitHub](https://github.com/luasampaio)  
  *Aqui Ã© o cantinho dos meus cÃ³digos! DÃ¡ uma espiada, mas cuidado com os bugs. ğŸ˜‚*

 ğŸ“¸ [Instagram](https://www.instagram.com/luasampaio/)  
  *Spoiler: mais fotos e menos linhas de cÃ³digo! ğŸ“·âœ¨*

*_texto original publicado em [medium.com](https://medium.com/@luciana.sampaio84/acumuladores-no-pyspark-o-que-s%C3%A3o-e-como-us%C3%A1-los-93e8ee4319a6)*